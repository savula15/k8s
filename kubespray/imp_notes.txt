=======================
UPDATED ON: 07/26/2018
=======================

https://github.com/kubernetes-incubator/kubespray#requirements

2 master
3 nodes

5 ubuntu VM's
==============
VMware VM's
with 2 HDDs ==> 1 with 50GB and anotehr with 450GB
sudo apt-get install python-minimal -y
sudo visudo ==> add NOPASSWD to %sudo group


1 ansible host
===============
ubuntu 16.06
python3
virtualenv


ssh key based authentication

Add following to each node in inventory/mycluster/hosts.ini
============================================================

ansible_user=test ansible_become=yes ansible_become_pass=ca$hc0w


ansible.cfg
============

add private_key_file=<pem file path>


kubespray/roles/kubernetes/preinstall/tasks/verify-settings.yml
=================================================================

hard-code kubelet_max_pods to 110


kubespray needs to set kubelet --fail-swap-on to false
=======================================================

inventory/mycluster/group_vars/k8s-cluster.yml setting: kubelet_fail_swap_on: false


Too Many nameservers error
===========================

Add setting docker_dns_servers_strict: false to inventory/mycluster/group_vars/all.yml


Kubectl on utility node
========================
Follow the steps below
Reference: https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

cp "admin.conf" under /etc/k8s/ on master node to "config" under ~/.kube/

export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config #you can also put this in ~/bash_profile

sudo kubectl config --kubeconfig=config set-cluster cluster.local --server=https://10.173.47.99:6443 --certificate-authority=/home/test/.kube/ca.pem

sudo kubectl config --kubeconfig=config set-credentials admin-cluster.local --certificate-authority=/home/test/.kube/ca.pem --client-key=/home/test/.kube/admin-node1-key.pem --client-certificate=/home/test/.kube/admin-node1.pem

sudo kubectl config --kubeconfig=config set-context admin-cluster.local --cluster=cluster.local --user=admin-cluster.local



helm and tiller caveats
========================
check "kubectl config current-context" once
and then do "helm init" which will deploy tiller into k8s cluster

In order for helm install to work:

kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'



